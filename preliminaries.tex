\chapter{Preliminaries}
\label{chap:preliminaries}

\chapterprecishere{%
  Maar ik maak steeds wat ik nog niet kan om het te leeren kunnen.\par\raggedleft---
  \textup{Vincent van Gogh}, The Complete Letters of Vincent Van Gogh, Volume Three}

Foundamental concepts in data science come from a variety of fields, including
mathematics, statistics, computer science, optimization theory, and information theory.
This chapter provides a brief overview of the computational, mathematical and statistical
concepts that are used in the rest of the book.

\section{Algorithms and data structures}

Algorithms are step-by-step procedures for solving a problem.  They are used to
manipulate data structures, which are ways of organizing data to solve problems.
They are realized in programming languages, which are formal languages that can be used
to express algorithms.

\subsection{Algoritmic paradigms}

Some techniques are used to solve a wide variety of problems.  They are called
algorithmic paradigms.  The most common ones are listed below.

\paragraph{Divide and conquer}  The problem is divided into smaller subproblems that are
solved recursively.  The solutions to the subproblems are then combined to give a solution
to the original problem.

\paragraph{Dynamic programming}  The problem is divided into overlapping subproblems, and
the solutions to the subproblems are only solved once. The subproblems are then optimized
to find the overall solution.

\paragraph{Greedy algorithms}  The problem is solved with incremental steps, each of which
is locally optimal.  The overall solution is not guaranteed to be optimal.

\subsection{Computational complexity}

Big-O notation, time and space complexity, NP-completeness.

\subsection{Data structures}

Arrays, linked lists, stacks, queues, trees, graphs, hash tables.

\section{Linear algebra}

\subsection{Vectors and matrices}

\subsection{Matrix decompositions}

\textcolor{red}{Verify!}

\paragraph{Singular value decomposition} The singular value decomposition (SVD) of a
matrix $A$ is a factorization of the form
\begin{equation}
  \label{eq:svd}
  A = U \Sigma V^T\text{,}
\end{equation}
where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal
matrix with non-negative real numbers on the diagonal.  The singular values are the
diagonal entries of $\Sigma$.

\paragraph{Eigenvalue decomposition}  The eigenvalue decomposition of a matrix $A$
is a factorization of the form
\begin{equation}
  \label{eq:eigdec}
  A = Q \Lambda Q^{-1}\text{,}
\end{equation}
where $Q$ is a square matrix whose columns are the eigenvectors of $A$, and
$\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of
$A$.

\paragraph{Cholesky decomposition}  The Cholesky decomposition of a positive-definite
matrix $A$ is a factorization of the form
\begin{equation}
  \label{eq:chol}
  A = L L^T\text{,}
\end{equation}
where $L$ is a lower triangular matrix with real and positive diagonal entries.

\paragraph{QR decomposition}  The QR decomposition of a matrix $A$ is a
factorization of the form
\begin{equation}
  \label{eq:qr}
  A = Q R\text{,}
\end{equation}
where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix.

\paragraph{LU decomposition}  The LU decomposition of a square matrix $A$ is a
factorization of the form
\begin{equation}
  \label{eq:lu}
  A = L U\text{,}
\end{equation}
where $L$ is a lower triangular matrix with unit diagonal entries and $U$ is
an upper triangular matrix.

\subsection{Eigenvalues and eigenvectors}

An eigenvalue of a square matrix $A$ is a scalar $\lambda$ such that there exists a
non-zero vector $\vec{v}$ satisfying
\begin{equation}
  \label{eq:eig}
  A \vec{v} = \lambda \vec{v}\text{.}
\end{equation}
The vector $\vec{v}$ is called an eigenvector of $A$ corresponding to $\lambda$.

\section{Probability}

\subsection{Axioms of probability}

The Kolmogorov axioms of probability are the foundation of probability theory.
They are
\begin{enumerate}
  \item The probability of an event $E$ is a non-negative real number, i.e. $P(A) \geq 0$;
  \item The probability of the sample space $\Omega$ is one, i.e. $P(\Omega) = 1$; and
  \item The probability of the union of disjoint events, $A \cap B = \emptyset$, is
    the sum of the probabilities of the events, i.e. $P(A \cup B) = P(A) + P(B)$.
\end{enumerate}

\subsection{Permutations and combinations}

\subsection{Conditional probability}

\subsection{Bayes' rule}

\subsection{Independence}

\subsection{Random variables}

\subsection{Probability distributions}

\subsection{Expectation and moments}

\section{Optimization}

\subsection{Minimization of convex functions}

\subsection{Gradient descent}

\subsection{Constraint optimization}

Techniques like Lagrange multipliers, penalty methods, and barrier methods are used to
handle constrained optimization problems in data science.

\subsection{Convex optimization}

Convex optimization problems, where the objective function and the constraints are convex,
have efficient algorithms that guarantee global optimality.

% \subsection{Gradient descent algorithm}
%
% Let $f(\vec{w})$, $f : \mathbb{R}^n \rightarrow \mathbb{R}$, be an objective function that
% we are trying to minimize.  We know that
% $f$ is convex, of class $\mathcal{C}^2$, and its gradient $\nabla f$ is Lipschitz continuous with Lipschitz
% constant $L > 0$.
%
% We want to show that $\lim_{t\rightarrow\infty} f(\vec{w}(t)) = f^{*}$ where $f^{*}$
% is the global minimum of $f$ and $$\vec{w}(t+1) = \vec{w}(t) - \alpha \nabla f(\vec{w}(t))\mbox{,}$$
% for any initial condition $\vec{w}(0)$ and $0 < \alpha \leq \frac{1}{L}$.
%
% Convexity implies that for any two points $\vec{v}$ and $\vec{w}$ in the domain of
% $f$, the line segment connecting them lies above the graph of $f$.  Mathematically, it
% means that $$f(t\vec{v} + (1 - t) \vec{w}) \leq t f(\vec{v}) + (1 - t)
% f(\vec{w})$$ for all $t \in [0, 1]$.
%
% The Lipschitz continuity condition means that the gradient of $f(\vec{w})$ does not change too rapidly.
% Formally, $$\left\|\nabla f(\vec{v}) - \nabla f(\vec{w})\right\| \leq L \|\vec{v} - \vec{w}\|\mbox{,}$$
% for all $\vec{v}$ and $\vec{w}$ in the domain of $f$.  This is a rather weak
% assumption, and it means that the gradient can not change arbitrarily fast.
%
% Since $f$ is convex and twice differentiable, its Hessian is a positive semidefinite
% matrix, and thus its norm is its largest eigenvalue.
%
% A consequence of the Lipschitz continuity for a $\mathcal{C}^2$ function $f$ is that for
% any $\vec{v}$ and $\vec{w}$, we have that
% \begin{equation}
%   \label{eq:lcg1}
%   \vec{v}^T \nabla^2 f(w) \vec{v} \leq L \|v\|^2\text{.}
% \end{equation}
% It means that the eigenvalues of the Hessian are bounded above by $L$.
%
% \paragraph{Descent lemma.}  For $f$, a the multivariate Taylor expansion is that
% $$f(w) = f(v)$$
