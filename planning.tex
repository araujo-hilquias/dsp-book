\chapter{Experimental planning}
\label{chap:planning}

\chapterprecishere{%
  All models are wrong, but some are useful.
  \par\raggedleft--- \textup{George E. P. Box}, Robustness in Statistics}

Once we have defined what an inductive problem is, we can start to think about how to
solve it in practical terms.

In this chapter, we present the experimental planning one can use in the data-driven
parts of a data science project.  \emph{Experimental planing}  in the context of data
science involves designing and organizing experiments to gather performance data
systematically in order to reach specific goals or test hypotheses.

The reason we need to plan experiments is that data science is experimental, i.e. we
usually lack a theoretical model that can predict the outcome of a given algorithm on a
given dataset  --- in other words, the quality of the solution given a certain approach.
This is why we need to run experiments to gather data and make
inferences from it.

There is not a single way to plan experiments, but there are some common steps that can
be followed to design a good experimental plan.  In this chapter, we present a
framework for experimental planning that can be used in most data science projects.

\section{Elements of an experimental plan}

There are important elements that should be considered when designing an experimental
plan.  These elements are:
\begin{itemize}
  \item \textbf{Hypothesis}: The main question that the experiment aims to validate.
  \item \textbf{Data}: The dataset that will be used in the experiment.
  \item \textbf{Solution search algorithm}\footnote{We use the term ``search'' because,
    the chosen algorithm aim at optimizing both the parameters of the data handling
    pipeline and the ones of the model}: Techniques that find a solution for the task.
    For us, a task includes both adjusting the appropriate data-handling
    pipeline and learning the model.
  \item \textbf{Performance measure}: The metric that will be used to evaluate the
    performance of the model.
\end{itemize}

A general example of a description of an experimental plan is ``What is the probability of
the technique $A$ to find a model that reaches a performance $X$ in terms of metric $Y$ in
the real-world given dataset $Z$ as training set (assuming $Z$ is a representative
dataset)?''

Another example is ``Is technique $A$ better than technique $B$ for finding a model that
predicts the output of dataset $C$ given $D$ as a training set in terms of metric $E$?''

We consider these two cases: \emph{estimating expected performance} and \emph{comparing
algorithms}.

\section{Estimating expected performance}

When dealing with a data-driven solution, the available data is a representation of the
real world.  So, we have to make the best use of the data we have to estimate how good our
solution is expected to be in production.

Obviously, the more data we use to search for a solution, the better the solution is
expected to be.  Thus, we use the whole dataset for deploying a solution.  But, what
method for preprocessing and learning should we use?  How well that technique is
expected to perform in the real world?

Let us say we fix a certain technique, let us call it $A$.  Let $M$ be the solution found
by $A$ using the whole dataset $D$.  If we assess $M$ using the whole dataset $D$, the
performance $\rho$ we get is optimistic.  This is because $M$ has been trained and tested
on the same data.

To estimate better the expected performance of $M$ in production, we can use the following
experimental plan.

\paragraph{Sampling strategy}  As any statistical evaluation, we need to generate samples
of the performance of the possible solutions that $A$ is able to obtain. To do so, we use
a sampling strategy to generate $r$ datasets $D_1, D_2, \ldots, D_r$ from $D$.  Each
dataset is further divided into a training set and a test set.  They must be disjoint.
The training set is thus used to find a solution --- $M_1, M_2, \ldots, M_r$ for each
training set --- and the test set is used to evaluate the performance --- $\rho_1, \rho_2,
\ldots, \rho_r$ for each test set --- of the solution.

The most common sampling strategy is the \emph{cross-validation}.  It assumes that data is
independent and identically distributed (i.i.d.).  The cross-validation technique divides
the dataset into $r$ folds randomly, with the same size.  Each part (fold) is used as a
test set once and as a training set $r-1$ times.  So, first we use as training set folds
$2, 3, \ldots, r$ and as test set fold $1$.  Then, we use as training set folds $1, 3,
\ldots, r$ and as test set fold $2$. And so on.  If possible, we should use repeated
cross-validation, where this process is repeated many times.  Also, when dealing with
classification problems, we should use stratified cross-validation, where the distribution
of the classes is preserved in each fold.

\begin{figurebox}[label=fig:plan-single]{Experimental plan for estimating expected performance of a solution.}
  \centering
  \begin{tikzpicture}
    \node [darkcircle] (data) at (0, 0) {Data};
    \node [block] (sampling) at (0, -2) {Sampling strategy};
    \path [line] (data) -- (sampling);

    \foreach \i in {1, 2, 4, 5} {
      \draw [dashed] (-7 + 2 * \i, -4.5) rectangle (-5.1 + 2 * \i, -3.5);
      \path [line] (sampling) -- (-6.1 + 2 * \i, -3.5);

      \node [smalldarkblock] (train\i) at (-6.4 + 2 * \i, -4) {Training};
      \node [smallblock] (test\i) at (-5.6 + 2 * \i, -4) {Test};

      \path [line] (-6.1 + 2 * \i, -4.5) -- (-6.1 + 2 * \i, -5.5);
    }
    \node [anchor=center] at (0, -4) {\dots};

    \draw [dashed] (-5, -5.5) rectangle (4.9, -10.5);

    \node [smalldarkblock, font=\small, inner sep=4pt] (train) at (-4, -7) {Training};
    \node [smallblock, inner sep=4pt] (test) at (-4, -9) {Test (no label)};

    \draw [dashed] (-3, -6) rectangle (3, -8);
    \node [anchor=south] at (0, -6.1) {Solution search algorithm};

    \node [block] (handling) at (-1.5, -7) {Data handling pipeline};
    \node [block] (learning) at (1.5, -7) {Machine learning};
    \node (model) at (4, -7) {%
      % bracket array with \theta and \phi
      $\left[
      \begin{array}{c}
        \phi \\
        \theta \\
      \end{array}
      \right]$
    };

    \path [line] (train) -- (handling);
    \path [line] (handling) -- (learning);
    \path [line, dashed] (3, -7) -- (model);

    \node [block] (preprocess) at (-1.5, -9) {Preprocessing};
    \node [block] (prediction) at (1.5, -9) {Prediction};

    \path [line, dashed] (handling) -- (preprocess);
    \path [line, dashed] (learning) -- (prediction);

    \path [line] (test) -- (preprocess);
    \path [line] (preprocess) -- (prediction);

    \node (performance) at (4, -9) {$\rho$};
    \path [line] (prediction) -- (performance);

    \node [smallblock, inner sep=4pt] (labels) at (-4, -10) {Test (labels)};
    \path [line] (labels) -- (4, -10) -- (performance);

    \node (perfs) at (-4.2, -12) {%
      $\left[
        \begin{array}{c}
          \rho_1 \\
          \rho_2 \\
          \vdots \\
          \rho_r \\
        \end{array}
      \right]$
    };

    \node [block] (visualization) at (-2, -12) {Visualization};
    \node [block] (summary) at (0.5, -12) {Summary statistics};
    \node [block] (hypothesis) at (3, -12) {Hypothesis test};

    \path [line, dashed] (-4.2, -10.5) -- (perfs);
    \path [line] (perfs) -- (-4.2, -13) -- (-2, -13) -- (visualization);
    \path [line] (-2, -13) -- (0.5, -13) -- (summary);
    \path [line] (0.5, -13) -- (3, -13) -- (hypothesis);
  \end{tikzpicture}
\end{figurebox}

A summary of the experimental plan for estimating expected performance is shown in
\cref{fig:plan-single}.

\section{Comparing strategies}

% vim: spell spelllang=en
