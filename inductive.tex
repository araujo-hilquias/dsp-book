\chapter{Statistical learning theory}


\chapterprecishere{%
  To  understand  God's  thoughts  we  must study statistics, for these are the measure of his purpose.
  \par\raggedleft--- \textup{Florence Nightingale}, her diary}

Recommended reading: IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999
An Overview of Statistical Learning Theory.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \draw[outline] (0,0) circle (30mm) node {};
    \node[below] at (0, 2.6) {artificial intelligence};
    \draw[outline] (0,-0.5) circle (25mm) node {};
    \node[below] at (0, 1.6) {machine learning};
    \draw[outline] (0,-1) circle (20mm) node {};
    \node[below] at (0, 0.5) {predictive learning};
    \draw[outline] (0,-1.5) circle (15mm) node {};
    \node[below] at (0, -1.0) {inductive learning};
    % \draw[outline] \verrist node {statistics};
    % \draw[outline] \verride node {domain expertise};
    % \draw[outline] \verrics node {computer science};
    % \node[anchor=north,above] at (0,2) {data science};
  \end{tikzpicture}
  \caption{
    \dots
  }
\end{figure}

\emph{Artificial intelligence} is a very broad field.  For the sake of our scope, it is
the field that studies algorithms that exhibit intelligent behavior.

\emph{Machine learning} is a subfield of artificial intelligence that studies algorithms
that enable computers to automatically learn and improve their performance on a task from
experience, without being explicitly programmed by a human being.

\emph{Predictive learning} is a machine learning paradigm that focuses on making
predictions about outcomes (sometimes about the future) based on historical data.
Depending on the reasoning behind the learning algorithms, the main predictive algorithms
are classified in either inductive or transductive.

Induction a type of reasoning that goes from specific instances to more general
principles. \emph{Inductive learning} is a machine learning approach that involves deriving
general rules from specific observations.

Maybe the most general (and useful) framework for predictive learning is Statistical
Learning Theory.

Transduction is out of the scope of this course.

\section{Hypothesis}

Consider the set
\begin{equation}
  \label{eq:training-set}
  \big\{(\vec{x}_i, y_i) : i = 1, \dots, n \big\}
\end{equation}
where each sample $i$ is associated with a feature vector $\vec{x}_i \in \mathcal{X}$ and a target variable
$y_i \in \mathcal{Y}$.  We assume that samples are random independent identically
distributed (i.i.d.) observations drawn according to $$P(x, y) = P(x) P(y | x)\text{.}$$
Both $P(x)$ and $P(y|x)$ are fixed but unknown.

\section{The learning problem}

Consider a \emph{learning machine} capable of generating a set of functions $f(x;
\theta) \equiv f_\theta(x)$, $\theta \in \Theta$ and $f_\theta : \mathcal{X} \rightarrow \mathcal{Y}$.
The problem of learning is that of choosing, among all possible $f_\theta$, the one that
predicts the target variable the best possible way.

In order to learn, we must first define the \emph{loss} (or discrepancy) $\mathcal{L}$
between the response $y$ to a given input $x$, drawn from $P(x, y)$, and the
response provided by the learning machine.

Then, given the \emph{risk function}
\begin{equation}
  \label{eq:risk}
  R(\theta) = \int \mathcal{L}(y, f_\theta(x))\, dP(x, y)\text{,}
\end{equation}
the goal is to find the function $f_\theta$ that minimizes $R(\theta)$
where the only available information is the \emph{training set} \eqref{eq:training-set}.

This formulation encompasses many specific problems. I focus on the two of them which I
believe are the most fundamental ones: \emph{binary data classification}\footnote{Vapnik
calls it \emph{pattern recognition}.} and \emph{regresssion estimation}\footnote{We are not talking about
\emph{regression analysis}.}.

\paragraph{Binary data classification task.}  In this task, the output $y$ take on
only two possible values, zero or one, and the functions $f_\theta$ are indicator
functions. For the loss
\begin{equation*}
  \mathcal{L}(y, f_\theta(x)) = \begin{cases}
    0 & \text{if } y = f_\theta(x) \\
    1 & \text{if } y \neq f_\theta(x)\text{,}
  \end{cases}
\end{equation*}
we aim at minimizing the risk $\eqref{eq:risk}$ which becomes the probability of
classification error.

\paragraph{Regression estimation task.} Let the outcome $y$ be a real value and
the \emph{regression} $r$ be $$r(x) = \int y\, dP(y|x) \text{.}$$
