\chapter{Fundamental data concepts}
\label{chap:data}

\chapterprecishere{The simple believes everything,
  \par\raggedleft but the prudent gives thought to his steps.
  \par\raggedleft--- \textup{Proverbs 14:15} (ESV)}

% \begin{itemize}
%   \item Variables, types etc
%   \item Records
%   \item Tidy data
% \end{itemize}

A useful start point for someone studying data science is the definition of the term
itself.

% \begin{mainbox}{Chapter remarks}
%   \boxsubtitle{Context}
%
%   \begin{itemize}
%     \item \dots
%   \end{itemize}
%
%   \boxsubtitle{Objectives}
%
%   \begin{itemize}
%     \item Understand the definition of data science.
%     \item Understand the main types of data.
%   \end{itemize}
%
%   \boxsubtitle{Takeways}
%
%   \begin{itemize}
%     \item Data science is a new science.
%     \item Data science is the study of computational methods to extract knowledge from
%       measurable phenomena.
%   \end{itemize}
% \end{mainbox}

For \textcite{Zumel2019}, \emph{``data science is a cross-disciplinary practice that draws
on methods from data engineering, descriptive statistics, data mining, machine learning,
and predictive analytics.''}  They compare the area with the operations research, stating
that data science focuses on implementing data-driven decisions and managing their
consequences.

\begin{slidebox}{Zumel and Mount's definition}{}
  \begin{itemize}
    \item Cross-disciplinary practice that draws on methods from data
    engineering, descriptive statistics, data mining, machine learning, and predictive
    analytics.
    \item Focuses on implementing data-driven decisions and managing their consequences.
  \end{itemize}
\end{slidebox}

\textcite{Wickham2023} state that \emph{``data science is an exciting discipline that
allows you to transform raw data into understanding, insight, and knowledge.''}

\begin{slidebox}{Wickham's definition}{}
  \begin{itemize}
    \item Transform raw data into understanding, insight, and knowledge.
    \item Not necessarily a definition; describes the purpose of data science.
  \end{itemize}
\end{slidebox}

I find the first definition too restrictive once new methods and techniques are always
under development.  We never know when new ``data-related'' methods will become obsolete
or a trend.  Also, \citeauthor{Zumel2019}'s view gives the impression that data science is a
operations research subfield.  Although I will not try to prove otherwise, I think it will
be much more useful to see it as an independent field of study.  Obviously, there will be
many intersections between both areas (and many other areas as well).  Because of such
intersections, I will try my best to keep definitions and
terms standardized throughout chapters, sometimes avoiding popular terms that may generate
ambiguities or confusion.

The second one is not really a definition.  However, it states clearly \emph{what} data
science enables us to do.  From these thoughts, let's define the term.

\begin{defbox}{}{ds}
  Data science is the study of computational methods to extract knowledge from
  measurable phenomena.
\end{defbox}

I want to highlight the meaning of some terms in this definition.  \emph{Computational methods} means
that data science methods use computers to handle data and perform the calculations.
\emph{Knowledge} means information that humans can easily understand or apply to solve
problems.  \emph{Measurable phenomena} are events or processes where raw data can be
quantified in some way.  \emph{Raw data} are data collected directly from some source and
that have not been subject to any other manipulation by a software program or a human
expert.  \emph{Data} is any piece of information that can be digitally stored.

\begin{slidebox}{My definition}{}
  \begin{itemize}
    \item Data science is the study of computational methods to extract knowledge from
      measurable phenomena.
    \item Computational methods use computers to handle data and perform the calculations.
    \item Knowledge is information that humans can easily understand or apply to solve
      problems.
    \item Measurable phenomena are events or processes where raw data can be quantified
      in some way.
    \item Raw data are data collected directly from some source and that have not been
      subject to any other manipulation by a software program or a human expert.
    \item Data is any piece of information that can be digitally stored.
  \end{itemize}
\end{slidebox}

\textcite{Kelleher2018} summarize very well the challenges data science takes up:
``extracting non-obvious and useful patterns from large data sets [\dots]; capturing,
cleaning, and transforming [\dots] data; [storing and processing] big [\dots] data sets;
and questions related to data ethics and regulation.''

\begin{slidebox}{Kelleher and Tierney's challenges}{}
  \begin{itemize}
    \item Extracting non-obvious and useful patterns from large data sets.
    \item Capturing, cleaning, and transforming data.
    \item Storing and processing big data sets.
    \item Questions related to data ethics and regulation.
  \end{itemize}
\end{slidebox}

Data science contrasts with conventional sciences.  Usually, a ``science'' is named after
its object of study.  Biology is the study of the life, Earth science studies the planet
Earth, and so on.  I argue that data science does not study data itself, but how we can
use them to understand a phenomenon.

Besides, the conventional scientific paradigm is
essentially model-driven: we observe a phenomenon related to the object of study, we
reason the possible explanation (the model or hypothesis), and we validate our hypothesis
(most of the time using data, though).  In data science, however, we extract the knowledge
directly and primarily from the data.  The expert knowledge and reasoning may be taken
into account, but we give data the opportunity to surprise us.

Thus, the objects of the
study in data science are the computational methods and processes that can extract
reliable and ethical knowledge from huge amounts of data.

\def\verrids{(0,0) circle (20mm)}
\def\verrist{(-2.5,0) circle (15mm)}
\def\verride {(2.5,0) circle (15mm)}
\def\verrics {(0,-2.5) circle (15mm)}

\begin{figurebox}[label=fig:myview]{My view of data science.}
  \centering
  \begin{tikzpicture}
    \begin{scope}
      \clip \verrids;
      \fill[filled] \verrist;
      \fill[filled] \verride;
      \fill[filled] \verrics;
    \end{scope}
    \draw[outline] \verrids node(ds) {};
    \draw[outline] \verrist node {statistics};
    \draw[outline, text width=27mm, text centered] \verride node {domain expertise philosophy};
    \draw[outline] \verrics node {computer science};
    \node[anchor=north,above] at (0, 1) {data science};
  \end{tikzpicture}
  \tcblower
    Data science is an entire new science.  Being a new science
    does not mean that its basis is built from the ground up.  Most of the subjects in
    data science come from other sciences, but its object of study (computational methods
    to extract knowledge from measurable phenomena) is particular enough to unfold
    new scientific questions -- such as data ethics, data collection, etc.
\end{figurebox}

\begin{slidebox}{Data science vs conventional sciences}{}
  \begin{itemize}
    \item Conventional sciences are model-driven: observation, hypothesis, and validation.
    \item In data science, we extract the knowledge directly and primarily from the data.
    \item Data science studies the computational methods and processes that can extract
      reliable and ethical knowledge from huge amounts of data.
  \end{itemize}
\end{slidebox}

\section{Definition components}

As expected, data science is not a isolated science.  It incorporates several concepts
from other fields and sciences.  In this section, I explain the basis of each component of
the provided definition.

\subsection{Phenomena}

Phenomenon is a term used to describe any observable event or process.  They are the
source we use to understand the world around us.  In general, we use our senses to
perceive phenomena.  To make sense of them, we use our knowledge and reasoning.

Philosophy is the study of knowledge and reasoning.  It is a very broad field of study
that has been divided into many subfields.  One of them is epistemology, which is the
study of knowledge.  Epistemology is the field of philosophy that studies how we can
acquire knowledge and how we can distinguish between knowledge and opinion.  In
particular, epistemology studies the nature of knowledge, justification, and the
rationality of belief.

Another important subfield in philosophy is ontology, which is the study of being.  It
studies the nature of being, existence, or reality.  Ontology is the field of philosophy
that studies what exists and how we can classify it.  In particular, ontology studies the
nature of categories, properties, and relations.

Finally, logic is the study of reasoning.  It studies the nature of reasoning and
argumentation.  In particular, logic studies the nature of inference, validity, and
fallacies.

\begin{slidebox}{Philosophy}{}
  \begin{itemize}
    \item Epistemology: the study of knowledge.
    \item Ontology: the study of being.
    \item Logic: the study of reasoning.
  \end{itemize}
\end{slidebox}

In the context of data science, we usually focus on phenomena from particular domain of
expertise.  For example, we may be interested in the phenomena related to the stock
market, the phenomena related to the weather, or the phenomena related to the human
health.  Thus, we need to understand the nature of the phenomena we are studying.

Thus, fully understading the phenomena we are tackling requires both a general knowledge
of epistemology, ontology, and logic, and a particular knowledge of the domain of
expertise.

Observe as well that we do not restrict ourselves to the ``qualitative'' understanding of
philosophy.  There are several computational methods that implements the concepts of
epistemology, ontology, and logic.  For example, we can use a computer to perform
deductive reasoning, to classify objects, or to validate an argument.  Also, we have
strong mathematical foundations and computational tools to organize categories, relations, and
properties.

The reason we need to understand the nature of the phenomena we are studying is that we
need to guarantee that the data we are collecting are relevant to the problem we are
trying to solve.  Incorrectly perception of the phenomena may lead to incorrect data
collection, which may lead to incorrect conclusions.

\begin{slidebox}{Phenomena}{}
  \begin{itemize}
    \item Phenomena are the source we use to understand the world around us.
    \item We use our senses to perceive phenomena.
    \item We use our knowledge and reasoning to make sense of them.
    \item Computational methods can be used to implement knowledge and reasoning.
    \item Phenomena are the source of data.
    \item We need to understand the nature of the phenomena we are studying.
    \item Incorrectly perception of the phenomena may lead to incorrect data collection,
      which may lead to incorrect conclusions.
  \end{itemize}
\end{slidebox}

\subsection{Measuments}

In data science, we are interested in measurable phenomena.  Measurable phenomena are
those that we can quantify in some way.  For example, the temperature of a room is a
measurable phenomenon because we can measure it using a thermometer.  The number of
people in a room is also a measurable phenomenon because we can count them.

When we quantify a phenomenon, we perform data collection.  Data collection is the process
of gathering data on targeted phenomenon in an established systematic way.
Once we have collected the data, we need to store them.  Data storage is the process of
storing data in a computer.

To perform those tasks, we need to understand the nature of data.  Data are any piece of
information that can be digitally stored.  Data can be stored in many different formats.
For example, we can store data in a spreadsheet, in a database, or in a text file.  We can
also store data in many different types.  For example, we can store data as numbers,
strings, or dates.

In data science, studying data types is important because they need to correctly reflect
the nature of the source phenomenon and be compatible with the computational methods we
are using.  Data types also restrict the operations we can perform on the data.

The foundation and tools to understand data types come from computer science.  Among the
subfields, I highlight:
\begin{itemize}
  \item Algorithms and data structures: the study of data types and the computational
    methods to manipulate them.
  \item Databases: the study of storing and retrieving data.
\end{itemize}

\begin{slidebox}{Measurable phenomena}{}
  \begin{itemize}
    \item Measurable phenomena are those that we can quantify in some way.
    \item Data collection is the process of gathering data on targeted phenomenon in an
      established systematic way.
    \item Data storage is the process of storing data in a computer.
    \item Data are any piece of information that can be digitally stored.
    \item Data can be stored in many different formats.
    \item Data can be stored in many different types.
    \item Data types need to correctly reflect the nature of the source phenomenon and be
      compatible with the computational methods we are using.
    \item Algorithms, data structures, and databases are important subfields of computer
      science when studying data collection, data storage, and data types.
  \end{itemize}
\end{slidebox}

\subsection{Knowledge extraction}

Once we have collected and stored the data, we need to extract knowledge from them.  In
data science, we use computational methods to extract knowledge from data.  These
computational methods may come from many different fields.  In particular, I highlight:
\begin{itemize}
  \item Statistics: the study of data collection, organization, analysis, interpretation,
    and presentation.
  \item Machine learning: the study of computational methods that can automatically learn from data.
  \item Artificial intelligence: the study of computational methods that can mimic human
    intelligence.
\end{itemize}

Also, many other fields contribute to the development of domain-specific computational
methods to extract knowledge from data.  For example, in the field of biology, we have
bioinformatics, which is the study of computational methods to analyze biological data.
Earth sciences have geoinformatics, which is the study of computational methods to
analyze geographical data.  And so on.

Each method has its own assumptions and limitations.  Thus, we need to understand the
nature of the methods we are using.  In particular, we need to understand the
expected input and output of them.  Whenever the available data do not match the
requirements of the method, we may perform data manipulation\footnote{%
  It is important to highlight that it is expected that some of the methods assumptions
  are not fully met.  These methods are usually robust enough to extract valuable
  knowledge even when data contain imperfections, errors and noise.  However, it is still
  useful to perform data manipulation to adjust data as much as possible.%
}.

Data manipulation mainly includes data cleaning, data transformation, and data
integration. Data cleaning is the process of detecting and correcting (or removing)
corrupt or inaccurate pieces of data.  Data transformation is the process of converting
data from one format or type to another.  Data integration is the process of combining
data from different sources into a single, unified view.

\begin{slidebox}{Knowledge extraction}{}
  \begin{itemize}
    \item We use computational methods to extract knowledge from data.
    \item Statistics, machine learning, and artificial intelligence are important
      sciences when studying knowledge extraction.
    \item Computational methods always have their own assumptions and limitations.
    \item Data manipulation is the process of adjusting data to the requirements of the
      computational methods.
    \item Data cleaning is the process of detecting and correcting (or removing) corrupt
      or inaccurate pieces of data.
    \item Data transformation is the process of converting data from one format or type
      to another.
    \item Data integration is the process of combining data from different sources into
      a single, unified view.
  \end{itemize}
\end{slidebox}

\section{Structured data}

As one expects, when we measure a phenomenon, the resulting data come in many different
formats.  For example, we can measure the temperature of a room using a thermometer.  The
resulting data will be a number.  We can assess English proficiency using an essay test.  The
resulting data will be a text.  We can register relationships between proteins and
their functions.  The resulting data will be a graph.

Thus, it is important to understand the nature of the data we are working with.

The most common data format is the \emph{structured data}.  Structured data are data that
are organized in a tabular format.  Each row in the table represents a single observation
and each column represents a variable that describes the observation.

We restrict the kind of information we store in each cell, i.e. the data type of each
measurement.  Each column has a data type.  The data type restrict the operations we can
perform on the data.  For example, we can perform arithmetic operations on numbers, but
not on text.

The most common classification of data types is Stevens’s types: nominal, ordinal,
interval, and ratio.  Nominal data are data that can be classified into categories.
Ordinal data are data that can be classified into categories and ordered.  Interval data
are data that can be classified into categories, ordered, and measured in fixed units.
Ratio data are data that can be classified into categories, ordered, measured in fixed
units, and have a true zero.  In practice, they difer on the logical and arithmetic operations
we can perform on them.

\begin{tablebox}{Stevens’s types}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cc}
    \toprule
    \textbf{Data type} & \textbf{Operations} \\
    \midrule
    Nominal & $=$ \\
    Ordinal & $=, <$ \\
    Interval & $=, <, +, - $ \\
    Ratio & $=, <, +, -, \times, \div$ \\
    \bottomrule
  \end{tabular}
\end{tablebox}

However, Stevens’s types do not exhaust all possibilities for data types.  For example,
probabilities are bounded at both ends, and thus do not tolerate arbitrary scale shifts.
\textcite{Paul1993} provide interesting insights about data types.  Although I do not
agree with all his points, I think it is a good reading.  In particular, I agree with
his criticism of statements that data types are evident from the data independent of the
questions asked.  The same data can be interpreted in different ways depending on the
context and the goals of the analysis.

However, I do not agree with the idea that good data analysis does not assume data types.
I think that data scientists should be aware of the data types they are working with and
how they affect the analysis.  With no bias, there is no learning.  There is no such a
thing as a ``bias-free'' analysis, the amount of possible combinations of assumptions
easily grows out of control.  The data scientist must take responsibility for the
consequences of their assumptions.  Good assumptions and hypothesis are a key part of the
data science methodology.

\begin{slidebox}{Structured data}{}
  \begin{itemize}
    \item Structured data are data that are organized in a tabular format.
    \item Each row in the table represents a single observation.
    \item Each column represents a variable that describes the observation.
    \item Each column has a data type.
    \item The data type restrict the operations we can perform on the data.
    \item The most common classification of data types is Stevens’s types: nominal,
      ordinal, interval, and ratio.
    \item Stevens’s types do not exhaust all possibilities for data types.
    \item Data scientists should be aware of the data types they are working with and
      how they affect the analysis.
    \item Inevitably, data scientists will make assumptions about the data types.
  \end{itemize}
\end{slidebox}

When we work with structured data, two concepts are very important: database normalization
and tidy data.  Database normalization is mainly focused on the data storage.  Tidy data is
mainly focused on the requirements of data for analysis.  Both concepts have their
mathematical foundations and tools for data manipulation.

\subsection{Database normalization}

Database normalization is the process of organizing the columns and tables of a relational
database to minimize data redundancy and improve data integrity.

Normal form is a state of a database that is free of certain types of data redundancy.
Before studying normal forms, we need to understand basic concepts in the database theory
and the basic operations in relational algebra.

\subsubsection{Relational database theory}

\paragraph{Projection}  The projection of a relation is the operation that returns a
relation with only the columns specified in the projection.  For example, if we have a
relation $X[A, B, C]$ and we perform the projection $\pi_{A, C}(X)$, we will get a
relation with only the columns $A$ and $C$, i.e. $X[A, C]$.

\paragraph{Join}  The (natural) join of two relations is the operation that returns a
relation with the columns of both relations.  For example, if we have two relations $S[U
\cup V]$ and $T[U \cup W]$, where $U$ is the common set of attributes, join $S \bowtie T$
of $S$ and $T$ is the relation with tuples $(u, v, w)$ such that $(u, v) \in S$ and $(u,
w) \in T$.  The generalized join is built up out of binary joins:  $\bowtie \left\{ R_1,
R_2, \dots, R_n \right\} = R_1 \bowtie R_2 \bowtie \dots \bowtie R_n$. Since the join
operation is associative and commutative, we can parenthesize however we want.

\paragraph{Functional dependency}  A functional dependency is a constraint between two
sets of attributes in a relation.  It is a statement that if two tuples agree on certain
attributes, then they must agree on another attribute.  Specifically, the \emph{functional
dependency} $U \to V$ holds in $R$ if and only if for every pair of tuples $t_1$ and $t_2$
in $R$ such that $t_1[U] = t_2[U]$, it is also true that $t_1[V] = t_2[V]$.

\paragraph{Multi-valued dependency}  A multi-valued dependency is a constraint between
two sets of attributes in a relation.  It is a statement that if two tuples agree on
certain attributes, then they must agree on another set of attributes.  Specifically, the
\emph{multi-valued dependency} $U \twoheadrightarrow V$ holds in $R$ if and only if $R =
R[UV] \bowtie R[UW]$.

\paragraph{Join dependency}  A join dependency is a constraint between subsets of
attributes (not necessarily disjoint) in a relation.  $R$ obeys the join dependency $*
\left\{ X_1, X_2, \dots, X_n \right\}$ if $R = \bowtie \left\{ R[X_1], R[X_2], \dots,
R[X_n] \right\}$.

\subsubsection{Normal forms}

\paragraph{First normal form (1NF)}  A relation is in 1NF if and only if all attributes
are atomic.  An attribute is atomic if it is not a set of attributes.  For example, the
relation $R[A, B, C]$ is in 1NF if and only if $A$, $B$, and $C$ are atomic.

\paragraph{Second normal form (2NF)}  A relation is in 2NF if and only if it is in 1NF
and every non-prime attribute is fully functionally dependent on the primary key.  A
non-prime attribute is an attribute that is not part of the primary key.  A primary key
is a set of attributes that uniquely identifies a tuple.  A non-prime attribute is fully
functionally dependent on the primary key if it is functionally dependent on the primary
key and not on any subset of the primary key.  For example, the relation $R[U \cup V]$ is
in 2NF if and only if $U \to X,~\forall X \in V$ and there is no $W \subset U$ such that
$W \to X,~\forall X \in V$.

\paragraph{Third normal form (3NF)}  A relation is in 3NF if and only if it is in 2NF
and every non-prime attribute is non-transitively dependent on the primary key.  A
non-prime attribute is non-transitively dependent on the primary key if it is not
functionally dependent on another non-prime attribute.  For example, the relation $R[U
\cup V]$ is in 3NF if and only if $U$ is the primary key and there is no $X \in V$ such
that $X \to Y,~\forall Y \in V$.

\paragraph{Boyce-Codd normal form (BCNF)}  A relation $R$ with attributes $X$ is in BCNF
if and only if it is in 2NF and for each nontrivial functional dependency $U \to V$ in
$R$, the functional dependency $U \to X$ is in $R$.  In other words, a relation is in BCNF
if and only if every functional dependency is the result of keys.

\paragraph{Fourth normal form (4NF)}  A relation $R$ with attributes $X$ is in 4NF if
and only if it is in 2NF and for each nontrivial multi-valued dependency $U \twoheadrightarrow
V$ in $R$, the functional dependency $U \to X$ is in $R$.  In other words, a relation is
in 4NF if and only if every multi-valued dependency is the result of keys.

\paragraph{Fifth normal form (5NF)} A relation $R$ with attributes $X$ is in
5NF\footnote{Also known as projection-join normal form.} if and only if it is in 2NF and
the set of key dependencies\footnote{Key dependency is a functional dependency in the form
$K \to X$.} of $R$ impllies each join dependency of $R$.  The 5NF guarantees that the
table cannot be decomposed without losing information (except by decompositions based on
keys).

Note that the ideia behind the definition of BCNF and 4NF are slightly different from the
5NF.  In fact, if we consider that for each key dependency implies a join dependency, the
relation is in the so-called overstrong projection-join normal
form\footfullcite{Fagin1979}.  Such a level of normalization does not improve data storage
or eliminate inconsistencies.  In practice, it means that if a relation is in 5NF,
careless joins --- i.e. those that violate a join dependency --- will produce
inconsistent results.

\paragraph{Example 1}  Consider the 1NF relation $R[A, B, C, D]$ with the functional
dependencies $A \to B,~B \to C,~C \to D$.  The relation is not in 3NF because $C$ is
transitively dependent on $A$.  To normalize it, we can decompose it into the
relations $R_1[A, B, C]$ and $R_2[C, D]$.  Now, $R_2$ is in 3NF and $R_1$ is in 2NF, but
not in 3NF.  We can decompose $R_1$ into the relations $R_3[A, B]$ and $R_4[B, C]$.
The original relation can be reconstructed by $\bowtie \left\{ R_2, R_3, R_4 \right\}$.

\paragraph{Example 2} Consider the 1NF relation $R[ABC]$\footnote{Here we abreviate ${A,
B, C}$ as $ABC$.} such that the primary key is the composite of $A$, $B$, and $C$.  The
relation is thus in the 4NF, as no column is a determinant of another column.  Suppose,
however, the following constraint: if $(a, b, c')$, $(a, b', c)$, and $(a', b, c)$ are in
$R$, then $(a, b, c)$ is also in $R$.  This can be illustrated if we consider $A$ as a
agent, $B$ as a product, and $C$ as a company.  If an agent $a$ represents companies $c$ and
$c'$, and product $b$ is in his portfolio, then assuming both companies make $b$, $a$
must offer $b$ from both companies.

The relation is not in 5NF, as the join dependency $* \left\{ AB, AC, BC \right\}$ is not
implied by the primary key.  (In fact, the only functional dependency is the trivial $ABC
\to ABC$.)  In this case, to avoid redundancies and inconsistencies, we must split the
relation into the relations $R_1[AB]$, $R_2[AC]$, and $R_3[BC]$.

It is iteresting to notice that in this case, the relation $R_1 \bowtie R_2$ might
contain tuples that do not make sense in the context of the original relation.  For
example, if $R_1$ contains $(a, b)$ and $R_2$ contains $(a, c')$, the join will contain
$(a, b, c')$, which might not be a valid tuple in the original relation if $(b, c')$ is
not in $R_3$.  \emph{This is very important to notice, as it is a common mistake to assume
that the join of the decomposed relations will always contain valid tuples.}

\paragraph{Example 3}  Consider the 1NF relation $R[A, B, C, D, E]$ with the functional
dependencies $A \to D$, $AB \to C$, and $B \to E$.  To make it 5NF, we can decompose it
into the relations $R_1[A, D]$, $R_2[A, B, C]$, and $R_3[B, E]$.  The original relation can
be reconstructed by $\bowtie \left\{ R_1, R_2, R_3 \right\}$.  However, unlike the
previous example, the join of the decomposed relations will always contain valid tuples.
The reason is that all join dependencies implied by the key dependencies are trivial.

\begin{slidebox}{Database normalization}{}
  \begin{itemize}
    \item Minimizes data redundancy.
    \item Improves data integrity.
    \item Semantics is expressed in terms of dependencies (functional, multivalued, join),
      which is usually not clear.
    \item Appropriate to store data.
  \end{itemize}
\end{slidebox}

\subsection{Tidy data}

Tidy data is a data format that provides a standardized way to organize data values within
a dataset.  It is based on the idea that a dataset is a collection of values, where:
\begin{itemize}
  \item Each value belongs to a variable.
  \item Each variable belongs to an observation.
  \item Each type of observational unit forms a table.
\end{itemize}

\begin{slidebox}{Tidy data}{}
  \begin{itemize}
    \item A dataset is a collection of values.
    \item Each value belongs to a variable.
    \item Each variable belongs to an observation.
    \item Each type of observational unit forms a table.
    \item Clear semantics with focus on only one view of the data.
    \item Appropriate to analyze data.
  \end{itemize}
\end{slidebox}

\subsection{Bridging normalization and tidyness}

Tidy data is related to the 3NF.  First and foremost, both concepts are not in conflict.

In data normalization, given a set of functional, multivalued and join dependencies, there
is only one normal form that is free of redundancy.  In tidy data,
\citeauthor{Wickham2023} also state that there is only one way to organize the given data.


\section{Unstructured data}

Unstructured data are data that do not have a predefined data model or are not organized
in a predefined manner.  For example, text, images, and videos are unstructured data.

Every unstructured dataset can be converted into a structured dataset.  However, the
conversion process is not always straightforward nor lossless.  For example, we can
convert a text into a structured dataset by counting the number of occurrences of each
word.  However, we lose the order of the words in the text.

The study of unstructured data is, for the moment, out of the scope of this book.
